{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans, SpectralClustering\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.image import imread\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "def delta(ck, cl):\n",
    "    values = np.ones([len(ck), len(cl)])*10000\n",
    "    \n",
    "    for i in range(0, len(ck)):\n",
    "        for j in range(0, len(cl)):\n",
    "            values[i, j] = np.linalg.norm(ck[i]-cl[j])\n",
    "            \n",
    "    return np.min(values)\n",
    "    \n",
    "def big_delta(ci):\n",
    "    values = np.zeros([len(ci), len(ci)])\n",
    "    \n",
    "    for i in range(0, len(ci)):\n",
    "        for j in range(0, len(ci)):\n",
    "            values[i, j] = np.linalg.norm(ci[i]-ci[j])\n",
    "            \n",
    "    return np.max(values)\n",
    "    \n",
    "def dunn(k_list):\n",
    "    \n",
    "    deltas = np.ones([len(k_list), len(k_list)])*1000000\n",
    "    big_deltas = np.zeros([len(k_list), 1])\n",
    "    l_range = list(range(0, len(k_list)))\n",
    "    \n",
    "    for k in l_range:\n",
    "        for l in (l_range[0:k]+l_range[k+1:]):\n",
    "            deltas[k, l] = delta(k_list[k], k_list[l])\n",
    "        \n",
    "        big_deltas[k] = big_delta(k_list[k])\n",
    "\n",
    "    di = np.min(deltas)/np.max(big_deltas)\n",
    "    return di"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('norm.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_req = df[['Stride Length (m)', 'Cadence(steps/min)', 'Leg Length (m)',\n",
    "       'Age(years)','classs']]\n",
    "x  = df_req[['Stride Length (m)', 'Cadence(steps/min)', 'Leg Length (m)',\n",
    "       'Age(years)']]\n",
    "labels = df.classs.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_tr,x_tt,y_tr,y_tt = train_test_split(x,labels,test_size=.30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.linalg import norm\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "class FCM:\n",
    "    \"\"\"Fuzzy C-means\n",
    "    \n",
    "    m: float, optional (default=2.0)\n",
    "        Exponent for the fuzzy partition matrix, specified as a\n",
    "        scalar greater than 1.0. This option controls the amount of\n",
    "        fuzzy overlap between clusters, with larger values indicating\n",
    "        a greater degree of overlap.\n",
    "    \n",
    "    u: array, shape = [n_samples, n_clusters]\n",
    "        Fuzzy partition array, returned as an array with n_samples rows\n",
    "        and n_clusters columns. Element u[i,j] indicates the degree of\n",
    "        membership of the jth data point in the ith cluster. For a given\n",
    "        data point, the sum of the membership values for all clusters is one.\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, n_clusters=10, max_iter=150, m=2, error=1e-5, random_state=42):\n",
    "        assert m > 1\n",
    "        self.u, self.centers = None, None\n",
    "        self.n_clusters = n_clusters\n",
    "        self.max_iter = max_iter\n",
    "        self.m = m\n",
    "        self.error = error\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def fit(self, X):\n",
    "        \n",
    "        self.n_samples = X.shape[0]\n",
    "        r = np.random.RandomState(self.random_state)\n",
    "        u = r.rand(self.n_samples, self.n_clusters)\n",
    "        u = u / np.tile(u.sum(axis=1)[np.newaxis].T, self.n_clusters)\n",
    "\n",
    "        r = np.random.RandomState(self.random_state)\n",
    "        self.u = r.rand(self.n_samples,self.n_clusters)\n",
    "        self.u = self.u / np.tile(self.u.sum(axis=1)[np.newaxis].T, self.n_clusters)\n",
    "\n",
    "        for iteration in range(self.max_iter):\n",
    "            u_old = self.u.copy()\n",
    "\n",
    "            self.centers = self.next_centers(X)\n",
    "            self.u = self._predict(X)\n",
    "\n",
    "            if norm(self.u - u_old) < self.error:\n",
    "                break\n",
    "\n",
    "\n",
    "    def next_centers(self, X):\n",
    "        \n",
    "        um = self.u ** self.m\n",
    "        return (X.T @ um / np.sum(um, axis=0)).T\n",
    "\n",
    "    def _predict(self, X):\n",
    "        \n",
    "        power = float(2 / (self.m - 1))\n",
    "        temp = cdist(X, self.centers) ** power\n",
    "        denominator_ = temp.reshape((X.shape[0], 1, -1)).repeat(temp.shape[-1], axis=1)\n",
    "        denominator_ = temp[:, :, np.newaxis] / denominator_\n",
    "\n",
    "        return 1 / denominator_.sum(2)\n",
    "\n",
    "    def predict(self, X):\n",
    "\n",
    "        if len(X.shape) == 1:\n",
    "            X = np.expand_dims(X, axis=0)\n",
    "\n",
    "        u = self._predict(X)\n",
    "        return np.argmax(u, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = FCM(n_clusters = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = a.fit(x.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = a.predict(x.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0004934284383001603"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dunn(a.centers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.80171894,  96.63679115,   0.71269004,  11.57552532],\n",
       "       [  0.87232181, 143.66037413,   0.57402528,   6.99943461]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette Coefficient: 0.161\n"
     ]
    }
   ],
   "source": [
    "print(\"Silhouette Coefficient: %0.3f\"\n",
    "      % silhouette_score(c.reshape(-1,1),labels, metric='euclidean'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "\n",
    "def purity_score(y_true, y_pred):\n",
    "    # compute contingency matrix (also called confusion matrix)\n",
    "    contingency_matrix = metrics.cluster.contingency_matrix(y_true, y_pred)\n",
    "    # return purity\n",
    "    return np.sum(np.amax(contingency_matrix, axis=0)) / np.sum(contingency_matrix) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6602564102564102"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "purity_score(labels,c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
